{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ae16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 通用工具 =====\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 数据处理与预处理 =====\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\n",
    "\n",
    "# ===== 聚类与降维 =====\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ===== 分类模型 =====\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ===== 超参数搜索 =====\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# ===== 模型评估 =====\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# ===== 可视化 =====\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 用于 3D 可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置文件路径\n",
    "\n",
    "input_file = r\"..\\data\\DryBeanDataset\\Dry_Bean_Dataset.xlsx\"\n",
    "df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查缺失值\n",
    "print(\"缺失值统计：\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 检查重复数据\n",
    "print(\"\\n重复数据统计：\")\n",
    "# 检查整行重复\n",
    "duplicates = df.duplicated()\n",
    "print(f\"整行完全重复的行数: {duplicates.sum()}\")\n",
    "if duplicates.sum() > 0:\n",
    "    print(\"\\n重复数据示例（显示两条）：\")\n",
    "    duplicate_rows = df[duplicates].head(2)\n",
    "    print(duplicate_rows)\n",
    "    \n",
    "    # 显示这些重复行的原始行号\n",
    "    print(\"\\n这些重复行的原始行号：\")\n",
    "    for idx in duplicate_rows.index:\n",
    "        print(f\"行号 {idx}:\")\n",
    "        print(df.iloc[idx])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# 删除重复数据\n",
    "df_cleaned = df.drop_duplicates()\n",
    "print(f\"\\n删除重复数据后，数据集大小从 {len(df)} 减少到 {len(df_cleaned)}\")\n",
    "\n",
    "# 添加ID列\n",
    "df_cleaned['ID'] = range(1, len(df_cleaned) + 1)\n",
    "print(\"\\n已添加ID列\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcee257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== One-Hot 编码 ==========\n",
    "if 'Class' in df_cleaned.columns:\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=None)  # drop='first' 可避免虚拟变量陷阱\n",
    "\n",
    "    # 对 'Class' 列进行编码\n",
    "    class_encoded = encoder.fit_transform(df_cleaned[['Class']]).astype(int)\n",
    "    class_encoded_cols = encoder.get_feature_names_out(['Class'])\n",
    "\n",
    "    # 将编码结果转为 DataFrame 并合并\n",
    "    class_encoded_df = pd.DataFrame(class_encoded, columns=class_encoded_cols, index=df_cleaned.index)\n",
    "    df_encoded = pd.concat([df_cleaned.drop(columns=['Class']), class_encoded_df], axis=1)\n",
    "\n",
    "# 将 'ID' 列移动到第一列\n",
    "col = df_encoded.pop('ID')\n",
    "df_encoded.insert(0, 'ID', col)\n",
    "\n",
    "# ========== 数据标准化 ==========\n",
    "# 获取数值型列（排除 ID 和 one-hot 编码列）\n",
    "numeric_columns = df_encoded.columns[0:16]\n",
    "if 'ID' in numeric_columns:\n",
    "    numeric_columns = numeric_columns.drop('ID')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numeric_columns] = scaler.fit_transform(df_encoded[numeric_columns])\n",
    "\n",
    "print(\"\\n已完成数据标准化（使用StandardScaler）\")\n",
    "print(\"标准化后的数据统计：\")\n",
    "print(df_encoded[numeric_columns].describe())\n",
    "\n",
    "# 保存处理后的数据\n",
    "output_file = 'processed_data_onehot.csv'\n",
    "df_encoded.to_csv(output_file, index=False)\n",
    "print(f\"\\n处理后的数据已保存到 {output_file}\")\n",
    "\n",
    "# ========== 检查每列的重复值 ==========\n",
    "print(\"\\n各列重复值统计：\")\n",
    "for column in df_cleaned.columns:\n",
    "    duplicate_count = df_cleaned[column].duplicated().sum()\n",
    "    print(f\"{column}: {duplicate_count} 个重复值\")\n",
    "\n",
    "# ========== 从 One-Hot 标签中提取整数标签 ==========\n",
    "data = pd.read_csv('processed_data_onehot.csv')\n",
    "\n",
    "# 假设最后7列是 One-Hot 编码的标签\n",
    "onehot_labels = data.iloc[:, -7:].to_numpy()\n",
    "y_true = onehot_labels.argmax(axis=1)\n",
    "data['encoded_label'] = y_true\n",
    "\n",
    "print(\"\\n添加整数标签后的数据预览：\")\n",
    "print(data.head())\n",
    "\n",
    "# 删除原来的 one-hot 标签列（保留最后一个标签列作为参考）\n",
    "data.drop(data.columns[-8:-1], axis=1, inplace=True)\n",
    "\n",
    "print(\"\\n删除部分 One-Hot 列后的数据预览：\")\n",
    "print(data.head())\n",
    "\n",
    "# 保存新的数据文件\n",
    "data.to_csv('processed_data_label_encoding.csv', index=False)\n",
    "print(\"\\n整数标签数据已保存为 'processed_data_label_encoding.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a17f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e57f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_visualize(data_file, output_dir='tsne_plots', perplexity=30, n_iter=1000, random_state=42):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # 提取特征和标签\n",
    "    feature_columns = [col for col in df.columns if col not in ['ID', 'encoded_label']]\n",
    "    if 'encoded_label' not in df.columns:\n",
    "        raise ValueError(\"数据中未找到 'encoded_label' 列。请检查输入数据格式。\")\n",
    "    \n",
    "    X = df[feature_columns].values\n",
    "    y = df['encoded_label'].astype(str).values\n",
    "\n",
    "    # t-SNE 降维\n",
    "    print(\"执行 t-SNE 降维...\")\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, n_iter=n_iter, random_state=random_state)\n",
    "    X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2D 图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in sorted(set(y)):\n",
    "        idx = y == label\n",
    "        plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=label, s=10)\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE 2D Visualization')\n",
    "    plt.xlabel('TSNE1')\n",
    "    plt.ylabel('TSNE2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'tsne_2d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 3D 图\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for label in sorted(set(y)):\n",
    "        idx = y == label\n",
    "        ax.scatter(X_embedded[idx, 0], X_embedded[idx, 1], X_embedded[idx, 2], label=label, s=5)\n",
    "    ax.set_title('t-SNE 3D Visualization')\n",
    "    ax.set_xlabel('TSNE1')\n",
    "    ax.set_ylabel('TSNE2')\n",
    "    ax.set_zlabel('TSNE3')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'tsne_3d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"PNG 格式的可视化图已保存至：{output_dir}\")\n",
    "    \n",
    "def tsne_cluster_visualize(X, labels, output_dir='tsne_cluster_plots', name='kmeans', perplexity=30, max_iter=1000, random_state=42):\n",
    "    print(f\"执行 t-SNE 降维并保存 PNG 图像: {name}\")\n",
    "\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, n_iter=max_iter, random_state=random_state)\n",
    "    X_embedded = tsne.fit_transform(X)\n",
    "    labels_str = labels.astype(str)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2D 图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in sorted(set(labels_str)):\n",
    "        idx = labels_str == label\n",
    "        plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=label, s=10)\n",
    "    plt.legend()\n",
    "    plt.title(f't-SNE 2D Clustering Visualization - {name}')\n",
    "    plt.xlabel('TSNE1')\n",
    "    plt.ylabel('TSNE2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{name}_tsne_2d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 3D 图\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for label in sorted(set(labels_str)):\n",
    "        idx = labels_str == label\n",
    "        ax.scatter(X_embedded[idx, 0], X_embedded[idx, 1], X_embedded[idx, 2], label=label, s=5)\n",
    "    ax.set_title(f't-SNE 3D Clustering Visualization - {name}')\n",
    "    ax.set_xlabel('TSNE1')\n",
    "    ax.set_ylabel('TSNE2')\n",
    "    ax.set_zlabel('TSNE3')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{name}_tsne_3d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"{name} 聚类的 PNG 可视化图已保存至：{output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd79c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'processed_data_label_encoding.csv'\n",
    "data_output = r\"tsne_plots\"\n",
    "tsne_visualize(data_file=data_path,output_dir=data_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634230fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(r\"processed_data_label_encoding.csv\")  \n",
    "X = df.iloc[:, 1:17].values \n",
    "y_true = df.iloc[:, -1].values\n",
    "\n",
    "kmeans = KMeans(n_clusters = 8, random_state=42)\n",
    "labels_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "gmm = GaussianMixture(n_components = 7, random_state=42)\n",
    "labels_gmm = gmm.fit_predict(X)\n",
    "\n",
    "hierarchical = AgglomerativeClustering(n_clusters = 6,  metric ='euclidean', linkage='ward')\n",
    "labels_hierarchical = hierarchical.fit_predict(X)\n",
    "\n",
    "\n",
    "\n",
    "tsne_cluster_visualize(X, labels_kmeans, output_dir='clustering_plots', name='KMeans')\n",
    "tsne_cluster_visualize(X, labels_gmm, output_dir='clustering_plots', name='GMM')\n",
    "tsne_cluster_visualize(X, labels_hierarchical, output_dir='clustering_plots', name='Hierarchical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4de119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取原始数据文件（请修改为你的文件路径）\n",
    "data = pd.read_csv(\"processed_data_label_encoding.csv\")  # 替换成你自己的文件名\n",
    "\n",
    "# 2. 使用 train_test_split 划分数据（test_size=0.3 表示30%作为测试集）\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. 保存训练集和测试集到新文件\n",
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "print(\"划分完成，训练集和测试集已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "142f7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,classes, name,result_dir=\"evaluation_results\"):\n",
    "\n",
    "    # Create result directory if not exists\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    df_test = pd.read_csv(\"test_data.csv\")  # 测试集路径\n",
    "    X_test = df_test.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y_test = df_test[\"encoded_label\"]\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Binarize labels for ROC curve if applicable\n",
    "    y_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "    # Predict the labels and probabilities\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Save metrics to file\n",
    "    with open(os.path.join(result_dir, \"metrics.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"模型在测试集上的评估结果：\\n\")\n",
    "        f.write(f\"Accuracy：{acc:.4f}\\n\")\n",
    "        f.write(f\"Precision：{prec:.4f}\\n\")\n",
    "        f.write(f\"Recall：{rec:.4f}\\n\")\n",
    "        f.write(f\"F1 score：{f1:.4f}\\n\\n\")\n",
    "\n",
    "\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "    with open(os.path.join(result_dir, \"metrics.txt\"), \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"各类 AUC 值：\\n\")\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            f.write(f\"  Class {classes[i]}: AUC = {roc_auc[i]:.2f}\\n\")\n",
    "\n",
    "        # Save ROC curve plot\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], label=f\"Class {classes[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title(f\"{name} Test Set - Multi-class ROC Curve\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(result_dir, \"roc_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'{name} Confusion Matrix - Per Class')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(os.path.join(result_dir, \"confusion_matrix_per_class.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e805a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'C': loguniform(1e-4, 1e4),\n",
    "        'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = LogisticRegression()\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model,classes=best_model.classes_,name='Logistic',result_dir='logistic_result')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016dd3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Random Search:\n",
      "{'C': 3448.3714871953325, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "best_model = logistic_regression(data_path=\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b36cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_features': [ 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30, 50],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = RandomForestClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name='random_forest',result_dir='random_forest_result')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f64613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters from Random Search:\n",
      "{'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "best_model = random_forest_classifier(data_path=\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (200,)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': loguniform(1e-4, 1e4),\n",
    "        'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = MLPClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name=\"mlp\" ,result_dir='mlp_result')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912661c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = mlp_classifier(data_path=\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d615c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def svm_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "    '''\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'C': loguniform(1e-4, 1e4),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree': [3, 4, 5],  # 适用于poly核\n",
    "        'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7)),  # 适用于rbf和sigmoid核\n",
    "        'coef0': [0, 0.1, 0.5, 1]  # 适用于poly和sigmoid核\n",
    "    }\n",
    "    '''\n",
    "    # 调整后的参数搜索空间\n",
    "    param_dist = {\n",
    "        'C': loguniform(1e-1, 1e2),  # 缩小C的范围\n",
    "        'kernel': ['linear', 'rbf'],  # 去掉poly和sigmoid核\n",
    "        'degree': [3, 4],  # 只保留适用于poly核的degree为3和4\n",
    "        'gamma': ['scale', 'auto'],  # 只保留常用的gamma选项\n",
    "        'coef0': [0, 0.1]  # 适用于poly和sigmoid核，只保留几个值\n",
    "    }\n",
    "\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = SVC(probability=True, random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name='svm',result_dir='svm_result')\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = svm_classifier(data_path=\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cac370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgboost_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [3, 6, 10, 15],\n",
    "        'learning_rate': loguniform(1e-4, 1e-1),\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'reg_alpha': [0, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.1, 1]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = XGBClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name='xgboost',result_dir='xgboost_result')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd2b8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Random Search:\n",
      "{'C': 3448.3714871953325, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/qiaoqian./anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 调用函数并训练模型\u001b[39;00m\n\u001b[1;32m      2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m logistic_regression(data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m best_model \u001b[38;5;241m=\u001b[39m mlp_classifier(data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m best_model \u001b[38;5;241m=\u001b[39m random_forest_classifier(data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m svm_classifier(data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m, in \u001b[0;36mmlp_classifier\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m    110\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m    111\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mbase_clf,\n\u001b[1;32m    112\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# 模型训练（直接用所有数据）\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# 输出最优模型和参数\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters from Random Search:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1951\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1951\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1952\u001b[0m         ParameterSampler(\n\u001b[1;32m   1953\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m   1954\u001b[0m         )\n\u001b[1;32m   1955\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    971\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m         clone(base_estimator),\n\u001b[1;32m    973\u001b[0m         X,\n\u001b[1;32m    974\u001b[0m         y,\n\u001b[1;32m    975\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    976\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    977\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    978\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    979\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    981\u001b[0m     )\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    985\u001b[0m     )\n\u001b[1;32m    986\u001b[0m )\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "best_model = xgboost_classifier(data_path=\"train_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
