{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 通用工具 =====\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 数据处理与预处理 =====\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\n",
    "\n",
    "# ===== 聚类与降维 =====\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ===== 分类模型 =====\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ===== 超参数搜索 =====\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# ===== 模型评估 =====\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# ===== 可视化 =====\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 用于 3D 可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置文件路径\n",
    "\n",
    "input_file = r\"..\\data\\DryBeanDataset\\Dry_Bean_Dataset.xlsx\"\n",
    "df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查缺失值\n",
    "print(\"缺失值统计：\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 检查重复数据\n",
    "print(\"\\n重复数据统计：\")\n",
    "# 检查整行重复\n",
    "duplicates = df.duplicated()\n",
    "print(f\"整行完全重复的行数: {duplicates.sum()}\")\n",
    "if duplicates.sum() > 0:\n",
    "    print(\"\\n重复数据示例（显示两条）：\")\n",
    "    duplicate_rows = df[duplicates].head(2)\n",
    "    print(duplicate_rows)\n",
    "    \n",
    "    # 显示这些重复行的原始行号\n",
    "    print(\"\\n这些重复行的原始行号：\")\n",
    "    for idx in duplicate_rows.index:\n",
    "        print(f\"行号 {idx}:\")\n",
    "        print(df.iloc[idx])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# 删除重复数据\n",
    "df_cleaned = df.drop_duplicates()\n",
    "print(f\"\\n删除重复数据后，数据集大小从 {len(df)} 减少到 {len(df_cleaned)}\")\n",
    "\n",
    "# 添加ID列\n",
    "df_cleaned['ID'] = range(1, len(df_cleaned) + 1)\n",
    "print(\"\\n已添加ID列\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcee257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== One-Hot 编码 ==========\n",
    "if 'Class' in df_cleaned.columns:\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=None)  # drop='first' 可避免虚拟变量陷阱\n",
    "\n",
    "    # 对 'Class' 列进行编码\n",
    "    class_encoded = encoder.fit_transform(df_cleaned[['Class']]).astype(int)\n",
    "    class_encoded_cols = encoder.get_feature_names_out(['Class'])\n",
    "\n",
    "    # 将编码结果转为 DataFrame 并合并\n",
    "    class_encoded_df = pd.DataFrame(class_encoded, columns=class_encoded_cols, index=df_cleaned.index)\n",
    "    df_encoded = pd.concat([df_cleaned.drop(columns=['Class']), class_encoded_df], axis=1)\n",
    "\n",
    "# 将 'ID' 列移动到第一列\n",
    "col = df_encoded.pop('ID')\n",
    "df_encoded.insert(0, 'ID', col)\n",
    "\n",
    "# ========== 数据标准化 ==========\n",
    "# 获取数值型列（排除 ID 和 one-hot 编码列）\n",
    "numeric_columns = df_encoded.columns[0:16]\n",
    "if 'ID' in numeric_columns:\n",
    "    numeric_columns = numeric_columns.drop('ID')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numeric_columns] = scaler.fit_transform(df_encoded[numeric_columns])\n",
    "\n",
    "print(\"\\n已完成数据标准化（使用StandardScaler）\")\n",
    "print(\"标准化后的数据统计：\")\n",
    "print(df_encoded[numeric_columns].describe())\n",
    "\n",
    "# 保存处理后的数据\n",
    "output_file = 'processed_data_onehot.csv'\n",
    "df_encoded.to_csv(output_file, index=False)\n",
    "print(f\"\\n处理后的数据已保存到 {output_file}\")\n",
    "\n",
    "# ========== 检查每列的重复值 ==========\n",
    "print(\"\\n各列重复值统计：\")\n",
    "for column in df_cleaned.columns:\n",
    "    duplicate_count = df_cleaned[column].duplicated().sum()\n",
    "    print(f\"{column}: {duplicate_count} 个重复值\")\n",
    "\n",
    "# ========== 从 One-Hot 标签中提取整数标签 ==========\n",
    "data = pd.read_csv('processed_data_onehot.csv')\n",
    "\n",
    "# 假设最后7列是 One-Hot 编码的标签\n",
    "onehot_labels = data.iloc[:, -7:].to_numpy()\n",
    "y_true = onehot_labels.argmax(axis=1)\n",
    "data['encoded_label'] = y_true\n",
    "\n",
    "print(\"\\n添加整数标签后的数据预览：\")\n",
    "print(data.head())\n",
    "\n",
    "# 删除原来的 one-hot 标签列（保留最后一个标签列作为参考）\n",
    "data.drop(data.columns[-8:-1], axis=1, inplace=True)\n",
    "\n",
    "print(\"\\n删除部分 One-Hot 列后的数据预览：\")\n",
    "print(data.head())\n",
    "\n",
    "# 保存新的数据文件\n",
    "data.to_csv('processed_data_label_encoding.csv', index=False)\n",
    "print(\"\\n整数标签数据已保存为 'processed_data_label_encoding.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a17f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "####visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_visualize(data_file, output_dir='tsne_plots', perplexity=30, n_iter=1000, random_state=42):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # 提取特征和标签\n",
    "    feature_columns = [col for col in df.columns if col not in ['ID', 'encoded_label']]\n",
    "    if 'encoded_label' not in df.columns:\n",
    "        raise ValueError(\"数据中未找到 'encoded_label' 列。请检查输入数据格式。\")\n",
    "    \n",
    "    X = df[feature_columns].values\n",
    "    y = df['encoded_label'].astype(str).values\n",
    "\n",
    "    # t-SNE 降维\n",
    "    print(\"执行 t-SNE 降维...\")\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, n_iter=n_iter, random_state=random_state)\n",
    "    X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2D 图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in sorted(set(y)):\n",
    "        idx = y == label\n",
    "        plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=label, s=10)\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE 2D Visualization')\n",
    "    plt.xlabel('TSNE1')\n",
    "    plt.ylabel('TSNE2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'tsne_2d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 3D 图\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for label in sorted(set(y)):\n",
    "        idx = y == label\n",
    "        ax.scatter(X_embedded[idx, 0], X_embedded[idx, 1], X_embedded[idx, 2], label=label, s=5)\n",
    "    ax.set_title('t-SNE 3D Visualization')\n",
    "    ax.set_xlabel('TSNE1')\n",
    "    ax.set_ylabel('TSNE2')\n",
    "    ax.set_zlabel('TSNE3')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'tsne_3d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"PNG 格式的可视化图已保存至：{output_dir}\")\n",
    "    \n",
    "def tsne_cluster_visualize(X, labels, output_dir='tsne_cluster_plots', name='kmeans', perplexity=30, max_iter=1000, random_state=42):\n",
    "    print(f\"执行 t-SNE 降维并保存 PNG 图像: {name}\")\n",
    "\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, n_iter=max_iter, random_state=random_state)\n",
    "    X_embedded = tsne.fit_transform(X)\n",
    "    labels_str = labels.astype(str)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2D 图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in sorted(set(labels_str)):\n",
    "        idx = labels_str == label\n",
    "        plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=label, s=10)\n",
    "    plt.legend()\n",
    "    plt.title(f't-SNE 2D Clustering Visualization - {name}')\n",
    "    plt.xlabel('TSNE1')\n",
    "    plt.ylabel('TSNE2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{name}_tsne_2d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 3D 图\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for label in sorted(set(labels_str)):\n",
    "        idx = labels_str == label\n",
    "        ax.scatter(X_embedded[idx, 0], X_embedded[idx, 1], X_embedded[idx, 2], label=label, s=5)\n",
    "    ax.set_title(f't-SNE 3D Clustering Visualization - {name}')\n",
    "    ax.set_xlabel('TSNE1')\n",
    "    ax.set_ylabel('TSNE2')\n",
    "    ax.set_zlabel('TSNE3')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{name}_tsne_3d.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"{name} 聚类的 PNG 可视化图已保存至：{output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd79c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'processed_data_label_encoding.csv'\n",
    "data_output = r\"tsne_plots\"\n",
    "tsne_visualize(data_file=data_path,output_dir=data_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634230fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(r\"processed_data_label_encoding.csv\")  \n",
    "X = df.iloc[:, 1:17].values \n",
    "y_true = df.iloc[:, -1].values\n",
    "\n",
    "kmeans = KMeans(n_clusters = 8, random_state=42)\n",
    "labels_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "gmm = GaussianMixture(n_components = 7, random_state=42)\n",
    "labels_gmm = gmm.fit_predict(X)\n",
    "\n",
    "hierarchical = AgglomerativeClustering(n_clusters = 6,  metric ='euclidean', linkage='ward')\n",
    "labels_hierarchical = hierarchical.fit_predict(X)\n",
    "\n",
    "\n",
    "\n",
    "tsne_cluster_visualize(X, labels_kmeans, output_dir='clustering_plots', name='KMeans')\n",
    "tsne_cluster_visualize(X, labels_gmm, output_dir='clustering_plots', name='GMM')\n",
    "tsne_cluster_visualize(X, labels_hierarchical, output_dir='clustering_plots', name='Hierarchical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4de119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取原始数据文件（请修改为你的文件路径）\n",
    "data = pd.read_csv(\"processed_data_label_encoding.csv\")  # 替换成你自己的文件名\n",
    "\n",
    "# 2. 使用 train_test_split 划分数据（test_size=0.3 表示30%作为测试集）\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. 保存训练集和测试集到新文件\n",
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "print(\"划分完成，训练集和测试集已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,classes, name,result_dir=\"evaluation_results\"):\n",
    "\n",
    "    # Create result directory if not exists\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    df_test = pd.read_csv(\"../test_data.csv\")  # 测试集路径\n",
    "    X_test = df_test.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y_test = df_test[\"encoded_label\"]\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Binarize labels for ROC curve if applicable\n",
    "    y_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "    # Predict the labels and probabilities\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Save metrics to file\n",
    "    with open(os.path.join(result_dir, \"metrics.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"模型在测试集上的评估结果：\\n\")\n",
    "        f.write(f\"Accuracy：{acc:.4f}\\n\")\n",
    "        f.write(f\"Precision：{prec:.4f}\\n\")\n",
    "        f.write(f\"Recall：{rec:.4f}\\n\")\n",
    "        f.write(f\"F1 score：{f1:.4f}\\n\\n\")\n",
    "\n",
    "\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "    with open(os.path.join(result_dir, \"metrics.txt\"), \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"各类 AUC 值：\\n\")\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            f.write(f\"  Class {classes[i]}: AUC = {roc_auc[i]:.2f}\\n\")\n",
    "\n",
    "        # Save ROC curve plot\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], label=f\"Class {classes[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title(f\"{name} Test Set - Multi-class ROC Curve\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(result_dir, \"roc_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'{name} Confusion Matrix - Per Class')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(os.path.join(result_dir, \"confusion_matrix_per_class.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a753cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'C': loguniform(1e-4, 1e4),\n",
    "        'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = LogisticRegression()\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model,classes=best_model.classes_,name='Logistic',result_dir='logistic_result')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def random_forest_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_features': [ 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30, 50],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = RandomForestClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name='random_forest',result_dir='random_forest_result')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def mlp_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (200,)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': loguniform(1e-4, 1e4),\n",
    "        'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = MLPClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name=\"mlp\" ,result_dir='mlp_result')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def svm_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "    '''\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'C': loguniform(1e-4, 1e4),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'degree': [3, 4, 5],  # 适用于poly核\n",
    "        'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7)),  # 适用于rbf和sigmoid核\n",
    "        'coef0': [0, 0.1, 0.5, 1]  # 适用于poly和sigmoid核\n",
    "    }\n",
    "    '''\n",
    "    # 调整后的参数搜索空间\n",
    "    param_dist = {\n",
    "        'C': loguniform(1e-1, 1e2),  # 缩小C的范围\n",
    "        'kernel': ['linear', 'rbf'],  # 去掉poly和sigmoid核\n",
    "        'degree': [3, 4],  # 只保留适用于poly核的degree为3和4\n",
    "        'gamma': ['scale', 'auto'],  # 只保留常用的gamma选项\n",
    "        'coef0': [0, 0.1]  # 适用于poly和sigmoid核，只保留几个值\n",
    "    }\n",
    "\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = SVC(probability=True, random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name='svm',result_dir='svm_result')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def xgboost_classifier(data_path=\"../train_data.csv\"):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # 特征与标签\n",
    "    X = df.drop(columns=[\"ID\", \"encoded_label\"])\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    # 参数搜索空间\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [3, 6, 10, 15],\n",
    "        'learning_rate': loguniform(1e-4, 1e-1),\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'reg_alpha': [0, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.1, 1]\n",
    "    }\n",
    "\n",
    "    # 定义模型和搜索器\n",
    "    base_clf = XGBClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 模型训练（直接用所有数据）\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # 输出最优模型和参数\n",
    "    print(\"Best Parameters from Random Search:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 获取训练好的最佳模型\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # 调用测试函数\n",
    "    evaluate_model(best_model, classes=best_model.classes_,name='xgboost',result_dir='xgboost_result')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用函数并训练模型\n",
    "best_model = logistic_regression(data_path=\"train_data.csv\")\n",
    "best_model = mlp_classifier(data_path=\"train_data.csv\")\n",
    "best_model = random_forest_classifier(data_path=\"train_data.csv\")\n",
    "best_model = svm_classifier(data_path=\"train_data.csv\")\n",
    "best_model = xgboost_classifier(data_path=\"train_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lab01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
